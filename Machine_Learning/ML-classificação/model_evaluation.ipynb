{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation\n",
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "iris = load_iris()\n",
    "logreg = LogisticRegression(max_iter=500)\n",
    "scores = cross_val_score(\n",
    "    logreg,\n",
    "    iris.data,\n",
    "    iris.target,\n",
    ")\n",
    "print(scores)\n",
    "print(f\"Score da validação cruzada: {scores.mean():.3f} +/- {scores.std():.3f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.96666667 1.         0.93333333 0.96666667 1.        ]\n",
      "Score da validação cruzada: 0.973 +/- 0.025\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stratified k-fold e outras estratégias"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits = 10, shuffle = True, random_state = 123)\n",
    "scores = cross_val_score(\n",
    "    logreg,\n",
    "    iris.data,\n",
    "    iris.target,\n",
    "    cv = kfold\n",
    ")\n",
    "print(scores)\n",
    "print(f\"Score da validação cruzada: {scores.mean():.3f} +/- {scores.std():.3f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.         0.93333333 0.93333333 1.         1.         0.93333333\n",
      " 0.93333333 1.         1.         0.86666667]\n",
      "Score da validação cruzada: 0.960 +/- 0.044\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Avaliar a performance do modelo depois da seleção do modelo\n",
    "***\n",
    "\n",
    "Solução usar nested cross-validation para evitar avaliação enviesada."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression(max_iter=500)\n",
    "\n",
    "# Create range of 20 candidate values para C\n",
    "C = np.logspace(0, 4, 20)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C = C)\n",
    "\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(\n",
    "    logistic,\n",
    "    hyperparameters,\n",
    "    cv=5, \n",
    "    n_jobs=-1,\n",
    "    verbose = 0,\n",
    ")\n",
    "\n",
    "# Conduct nested cross-validation and output the average score\n",
    "cv_results = cross_val_score(\n",
    "    grid_search, \n",
    "    features, \n",
    "    target,\n",
    ")\n",
    "scores = cv_results\n",
    "print(f\"A média da acurácia no cross-validation é: {scores.mean():.3f} +/- {scores.std():.3f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A média da acurácia no cross-validation é: 0.967 +/- 0.030\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Discussão sobre nested cross-validation.\n",
    "\n",
    "Nested cross-validation durante a seleção do modelo é um conceito muito bem complicado para muitas pessoas entenderem da primeira vez. Lembre-se que em k-fold cross-validation, nós treinamos o modelo em $k - 1$ folds dos dados, usamos este modelo para fazer predições no fold remanescente e avaliamos o nosso melhor modelo em como as predições estão boas comparadas com os valores reais. Nós então repetimos esse processo $k$ vezes.\n",
    "\n",
    "Na procura pelo melhor modelo, podemos usar **GridSearchCV** ou **RandomizedSearchCV**, nós usamos cross-validation para avaliar que hiperparâmetros, produzem os melhores modelos. Contudo, surge um problema, já que usamos os dados para selecionar os melhores hiperparâmetros, não podemos usar os mesmos dados para avaliar a performance do modelo. A solução? Envelopar a cross-validation usada na procura do modelo em outra cross-validation. Nesta cross-validation aninhada(nested), a cross-validation \"inner\" (mais interna- primeiro nível) seleciona o melhor modelo, enquanto a cross-validation \"outer\" (nível mais externo) nos proporciona uma avaliação do modelo não enviesada da performance. Na nossa solução, a cross-validation \"inner\"é o objeto do GridSearchCV, enquanto nos envelopamos a cross-validation \"outer\" usando cross_val_score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para tirar a confusão que esse processo traz. Façamos o seguinte experimento. Primeiramente, vamos setar verbose=1, no grid search para vermos o que está acontecendo, na procura pelo do melhor modelo através da seleção dos melhores parâmetros."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    KFold)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = logistic\n",
    "print(\"Os parâmetros do modelo são:\")\n",
    "for parameter in model.get_params():\n",
    "    print(f\"{parameter}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Os parâmetros do modelo são:\n",
      "C\n",
      "class_weight\n",
      "dual\n",
      "fit_intercept\n",
      "intercept_scaling\n",
      "l1_ratio\n",
      "max_iter\n",
      "multi_class\n",
      "n_jobs\n",
      "penalty\n",
      "random_state\n",
      "solver\n",
      "tol\n",
      "verbose\n",
      "warm_start\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "cv_inner = StratifiedKFold(\n",
    "    n_splits = 5,\n",
    "    shuffle=True,\n",
    "    random_state=0)\n",
    "\n",
    "model.set_params(max_iter=1000)\n",
    "gridsearch = GridSearchCV(\n",
    "    logistic,\n",
    "    hyperparameters,\n",
    "    cv = cv_inner,\n",
    "    verbose = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos rodar o gridsearch, com a nossa \"inner\" cross-validation usada para achar o melhor modelo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "best_model = gridsearch.fit(features, target)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Da saída da célula, vemos que a \"inner\" cross-validation treinou 20 candidatos, totalizando 100 modelos. Agora vamos colocar esse gridsearch dentro de outra validação cruzada."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "%%time\n",
    "cv_outer = KFold(\n",
    "    n_splits = 5,\n",
    "    shuffle=True, \n",
    "    random_state=0\n",
    "    )\n",
    "\n",
    "scores=cross_val_score(\n",
    "    gridsearch, \n",
    "    features,\n",
    "    target,\n",
    "    cv=cv_outer,\n",
    "    verbose=1, \n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "CPU times: user 41.4 s, sys: 143 ms, total: 41.6 s\n",
      "Wall time: 41.5 s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   41.5s finished\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A saída nos mostra que treinamos a inner cross-validation treinou 20 modelos 5 vezes para achar o melhor modelo e então esse modelo foi avaliado em uma \"outer\" cross-validation, criando um total de 500 modelos treinados."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "pipeline"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Queremos achar o melhor C através de um grid-search onde C possa tomar os seguintes valores C = [0.1, 1, 10]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "pipeline.steps"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('scaler', StandardScaler()), ('classifier', LogisticRegression())]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "print(\"Os parâmetros do pipeline são:\")\n",
    "for parameter in pipeline.get_params():\n",
    "    print(parameter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Os parâmetros do pipeline são:\n",
      "memory\n",
      "steps\n",
      "verbose\n",
      "scaler\n",
      "classifier\n",
      "scaler__copy\n",
      "scaler__with_mean\n",
      "scaler__with_std\n",
      "classifier__C\n",
      "classifier__class_weight\n",
      "classifier__dual\n",
      "classifier__fit_intercept\n",
      "classifier__intercept_scaling\n",
      "classifier__l1_ratio\n",
      "classifier__max_iter\n",
      "classifier__multi_class\n",
      "classifier__n_jobs\n",
      "classifier__penalty\n",
      "classifier__random_state\n",
      "classifier__solver\n",
      "classifier__tol\n",
      "classifier__verbose\n",
      "classifier__warm_start\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "param_grid = {\"classifier__C\" : [0.1, 1, 10]}\n",
    "model = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid\n",
    ").fit(X, y)\n",
    "model.best_params_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'classifier__C': 10}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model.best_estimator_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('classifier', LogisticRegression(C=10))])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "print(\"A melhor acurácia achada foi: \")\n",
    "model.best_score_"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A melhor acurácia achada foi: \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model.cv_results_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01159291, 0.01415353, 0.0206151 ]),\n",
       " 'std_fit_time': array([0.00280776, 0.00246526, 0.00227269]),\n",
       " 'mean_score_time': array([0.00079789, 0.00063887, 0.00074949]),\n",
       " 'std_score_time': array([1.59878287e-04, 6.14962077e-05, 2.00149499e-04]),\n",
       " 'param_classifier__C': masked_array(data=[0.1, 1, 10],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier__C': 0.1},\n",
       "  {'classifier__C': 1},\n",
       "  {'classifier__C': 10}],\n",
       " 'split0_test_score': array([0.83333333, 0.96666667, 1.        ]),\n",
       " 'split1_test_score': array([0.96666667, 1.        , 1.        ]),\n",
       " 'split2_test_score': array([0.93333333, 0.93333333, 0.93333333]),\n",
       " 'split3_test_score': array([0.9       , 0.9       , 0.93333333]),\n",
       " 'split4_test_score': array([1., 1., 1.]),\n",
       " 'mean_test_score': array([0.92666667, 0.96      , 0.97333333]),\n",
       " 'std_test_score': array([0.05734884, 0.03887301, 0.03265986]),\n",
       " 'rank_test_score': array([3, 2, 1], dtype=int32)}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* RandomizedSearchCV has a fixed computation budget through its n_iter parameter.\n",
    "* GridSearchCV can become very computationally intensive when the number of parameters grows.\n",
    "* both GridSearchCV and RandomizedSearchCV have the attributes cv_results_ and best_params_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}